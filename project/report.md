# Hadoop and Spark Clusters with Raspberry Pi

Akshay Kowshik, akowshik@iu.edu [fa19-516-162](https://github.com/cloudmesh-community/fa19-516-150) working on Hadoop.

Daivik Uggehalli Dayanand, daugge@iu.edu [fa19-516-158](https://github.com/cloudmesh-community/fa19-516-162) working on Spark.

## Abstract
Deployment of Hadoop and Spark on Raspberry Pi Clusters which involves:
* Developing a REST service that submits Hadoop/Spark jobs to the cluster remotely.
* Creating a cluster with as many nodes as we have SD cards for.
* Switching between Hadoop and Spark.

## Introduction

## Related Work

## Architecture

## Technologies used
* cm-burn
* cloudmesh-inventory
* Python
* REST
* HDFS
* Hadoop
* Spark

## Workbreakdown

## Progress


## Benchmark and Evaluation 
* Use PyTest 

## Conclusion

## References
