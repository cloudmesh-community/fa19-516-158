# Hadoop and Spark Clusters With Raspberry Pi

Akshay Kowshik, akowshik@iu.edu, [fa19-516-150](https://github.com/cloudmesh-community/fa19-516-150) working on Hadoop

Daivik Uggehalli Dayanand, daugge@iu.edu, [fa19-516-158](https://github.com/cloudmesh-community/fa19-516-158) working on Spark

## Abstract

Deployment of Hadoop and Spark on Raspberry Pi Clusters which involves:
* Installing Hadoop and Spark on Clusters
* Developing a REST service that submits Hadoop/Spark jobs to the cluster remotely
* Creating a cluster with as many nodes as we have SD cards for
* Switching across clusters between Hadoop and Spark using a REST Service

## Introduction

## Related Work

## Architecture

## Technologies used

* cm-burn
* cloudmesh-inventory
* Python
* REST
* HDFS
* Hadoop
* Spark

## Work Breakdown

## Progress

* Burning SD Cards using Etcher on Windows
* Read about battery operated clock for Raspberry Pis, NTP and RTC
* Setting up the cluster

## Benchmark and Evaluation 

* Develop a test program to review Hadoop and Spark on Clusters
* Use PyTest

## Conclusion

## References

* <https://raspberrytips.com/install-raspbian-raspberry-pi/>
* <https://raspberrytips.com/raspberry-pi-cluster/>
* <https://dev.to/awwsmm/building-a-raspberry-pi-hadoop-spark-cluster-8b2>
* <https://dqydj.com/raspberry-pi-hadoop-cluster-apache-spark-yarn/>
* <https://www.mocomakers.com/building-a-raspberry-pi-cluster-with-apache-spark/>
