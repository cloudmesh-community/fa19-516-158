# Hadoop and Spark Clusters With Raspberry Pi

Akshay Kowshik, akowshik@iu.edu, [fa19-516-162](https://github.com/cloudmesh-community/fa19-516-150) working on Hadoop

Daivik Uggehalli Dayanand, daugge@iu.edu, [fa19-516-158](https://github.com/cloudmesh-community/fa19-516-162) working on Spark

## Abstract
Deployment of Hadoop and Spark on Raspberry Pi Clusters which involves:
* Installing Hadoop and Spark on Clusters
* Developing a REST service that submits Hadoop/Spark jobs to the cluster remotely
* Creating a cluster with as many nodes as we have SD cards for
* Switching across clusters between Hadoop and Spark using a REST Service

## Introduction

## Related Work

## Architecture

## Technologies used
* cm-burn
* cloudmesh-inventory
* Python
* REST
* HDFS
* Hadoop
* Spark

## Work Breakdown

## Progress
* Burning SD Cards using Etcher on Windows
* Read about battery operated clock for Raspberry Pis, NTP and RTC
* Setting up the cluster

## Benchmark and Evaluation 
* Develop a test program to review Hadoop and Spark on Clusters
* Use PyTest

## Conclusion

## References
